<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="UTF-8">
  <title>Making visible</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.6.0/p5.js"></script>
  <!-- Riferimento online della risorsa p5 in js-->
  <script language="javascript" type="text/javascript" src="assets/exif.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script type="text/javascript" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"/></script>
  <script type="text/javascript" src="https://download.affectiva.com/js/3.1/affdex.js"/></script>
  <script language="javascript" type="text/javascript" src="sketch_merge.js"></script>
</head>

<body>
<button id="start" onclick="onStart()">Inizia</button>


<div id="preview" style="border: 1px solid; width: 400px; height: 300px; float: left; margin: 10px; padding: 0;">

</div>

<script>
// Link a div for previewing the camera feed
var divRoot = $("#preview")[0];

// Configuration
var width = 400;
var height = 300;
var faceMode = affdex.FaceDetectorMode.LARGE_FACES; // type of face recognize (Large is for adult)
var faces_count;
var emotion = [];
var appearances = [];

//Construct a CameraDetector
var detector = new affdex.CameraDetector(divRoot, width, height, faceMode);


//Enable detection of all Expressions, Emotions and Emojis classifiers.
detector.detectAllAppearance();
detector.detectAllEmotions();

//function executes when Start button is pushed.
function onStart() {
  if (detector && !detector.isRunning) {
    detector.start();
  }
}

//function executes when frameCount has arrived to the max value.
function onStop() {
	if (detector && detector.isRunning) {
		detector.removeEventListener();
		detector.stop();
	}
};

//Add a callback to notify when the detector is initialized and ready for runing.
detector.addEventListener("onInitializeSuccess", function() {
  //The following two objects are oart of the Affectiva SDK
  //Display canvas instead of video feed because we want to draw the feature points on it
  $("#face_video_canvas").css("display", "block");
  $("#face_video").css("display", "none");
});

//Add a callback to receive the results from processing an image.
//The faces object contains the list of the faces detected in an image.
//Faces object contains probabilities for all the different expressions, emotions and appearance metrics
detector.addEventListener("onImageResultsSuccess", function(faces, image, timestamp) {
  if (faces.length > 0) {
//    var resultsHtml = "";
//    resultsHtml += "Number of faces found: " + faces.length;
//    resultsHtml += "<br>Appearances: " + JSON.stringify(faces[0].appearance);
//    resultsHtml += "<br>Emotions: " + JSON.stringify(faces[0].emotions);
    faces_count = faces.length;
    appearances[timestamp] == JSON.stringify(faces[0].appearance);
    emotion += JSON.stringify(faces[0].emotions);
    //var htmlToDisplay = resultsHtml.replace(/,/g,",<br>");	// make output nicer
    //$('#results').html(htmlToDisplay);
    //drawFeaturePoints(image, faces[0].featurePoints);
    //when the face is recognize the image show off
    $('#preview').css('display', 'none');
  }
});

/*
//Draw the detected facial feature points on the image
function drawFeaturePoints(img, featurePoints) {
  var contxt = $('#face_video_canvas')[0].getContext('2d');

  var hRatio = contxt.canvas.width / img.width;
  var vRatio = contxt.canvas.height / img.height;
  var ratio = Math.min(hRatio, vRatio);

  contxt.strokeStyle = "#FFFFFF";
  for (var id in featurePoints) {
    contxt.beginPath();
    contxt.arc(featurePoints[id].x,
    featurePoints[id].y, 2, 0, 2 * Math.PI);
    contxt.stroke();

  }
}
*/

</script>
</body>
</html>
